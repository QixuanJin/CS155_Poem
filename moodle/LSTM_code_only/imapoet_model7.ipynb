{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, concatenate, BatchNormalization, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pickle import dump\n",
    "from pickle import load \n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ready-to-run shakespeare training data \n",
    "X = load(open(\"X_training_shakespeare_final.h5\", \"rb\"))\n",
    "y = load(open(\"y_training_shakespeare_final.h5\", \"rb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = load(open(\"mapping.pkl\", \"rb\"))\n",
    "vocab_size = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using pretrained layers in Model 5 to initialize Model 7 \n",
    "prev_model = load_model(\"model5_6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See model_dense diagram for visualization\n",
    "# Dual-input LSTM + Dense architecture (Model 7)\n",
    "# Takes in same input but processes with two channels first\n",
    "# Char_1 channel: new LSTM + 3 Dense layers \n",
    "# Char_2 channel: same architecture as Model 5 \n",
    "# After merge with concatenation, we added another 120 units \n",
    "# Dense layer before final softmax Dense layer. \n",
    "\n",
    "char_1_input = Input(shape = (X.shape[1], X.shape[2]), name='char_input_1')\n",
    "char_2_input = Input(shape = (X.shape[1], X.shape[2]), name=\"char_input_2\")\n",
    "\n",
    "x_1 = LSTM(120, name='x_1')(char_1_input)\n",
    "d_1 = Dropout(0.1) (x_1)\n",
    "\n",
    "x_2 = Dense(120, name='dense_1') (d_1)\n",
    "b_1 = BatchNormalization()(x_2)\n",
    "a_1 = Activation('relu')(b_1)\n",
    "\n",
    "x_3 = Dense(120, name='dense_2') (a_1)\n",
    "b_2 = BatchNormalization()(x_3)\n",
    "a_2 = Activation('relu')(b_2)\n",
    "\n",
    "x_4 = Dense(120, name='dense_3') (a_2)\n",
    "b_3 = BatchNormalization()(x_4)\n",
    "a_3 = Activation('relu')(b_3)\n",
    "\n",
    "extra_output = Dense(vocab_size, activation='softmax', name='extra')(a_3)\n",
    "\n",
    "y_1 = LSTM(120, return_sequences=True, name='y_1') (char_2_input)\n",
    "d_2 = Dropout(0.1) (y_1)\n",
    "y_2 = LSTM(120, return_sequences=True, name='y_2') (d_2)\n",
    "y_3 = LSTM(120, name='y_3') (y_2)\n",
    "\n",
    "z = concatenate([y_3, a_3])\n",
    "z_1 = Dense(120, activation='relu', name='dense_4')(z)\n",
    "main_output = Dense(vocab_size, activation='softmax', name='softmax')(z_1)\n",
    "\n",
    "model = Model(inputs=[char_1_input, char_2_input], outputs=[main_output, extra_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model\n",
    "plot_model(model, to_file='model_dense.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get layers from Model 5\n",
    "prev_LSTM_1 = prev_model.layers[0]\n",
    "prev_LSTM_2 = prev_model.layers[2]\n",
    "prev_Dense = prev_model.layers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "model.get_layer('y_1').set_weights(prev_LSTM_1.get_weights())\n",
    "model.get_layer('y_2').set_weights(prev_LSTM_2.get_weights())\n",
    "model.get_layer('softmax').set_weights(prev_Dense.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "91478/91478 [==============================] - 397s 4ms/step - loss: 3.7520 - softmax_loss: 2.4330 - extra_loss: 2.6381 - softmax_acc: 0.4095 - extra_acc: 0.2767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb31deef98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 7 training conditions\n",
    "# Saved model every 5 to 10 epochs\n",
    "# Total 20 epochs\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], loss_weights=[1, 0.2])\n",
    "model.fit([X, X], [y, y], epochs=5, batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
