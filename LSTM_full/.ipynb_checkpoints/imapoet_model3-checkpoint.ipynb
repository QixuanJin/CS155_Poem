{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense \n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pickle import dump\n",
    "from pickle import load \n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in a text file and returns contents as a string\n",
    "def load_raw(filename): \n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the raw text file as a string\n",
    "file1 = 'shakespeare.txt'\n",
    "raw_text = load_raw(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove header for first poem \n",
    "process_text = re.sub(\"                   1\", \"\", raw_text)\n",
    "# Remove any extra spaces\n",
    "process_text = re.sub(\" +\", \" \", process_text)\n",
    "# Remove header for remaining poems \n",
    "# Split along newlines and any remaining numbers\n",
    "process_text = re.split(\"\\n\\n\\n                   .|\\n\\n\\n.|\\n|\\d\", process_text)\n",
    "# Remove any empty strings that resulted from the split\n",
    "process_text = list(filter(None, process_text))\n",
    "\n",
    "# Check result\n",
    "print(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make single string of all the characters we want to keep\n",
    "process_string = \"\".join(process_text)\n",
    "# Split string into individual characters\n",
    "char_text = list(process_string)\n",
    "# Remove any empty characters\n",
    "char_text = list(filter(None, char_text))\n",
    "\n",
    "# Check length of characters\n",
    "print(len(char_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the characters into consecutive sequences of length 41 \n",
    "# 40 for training, 1 for prediction\n",
    "# Each sequence has an offset of one\n",
    "length = 40 + 1\n",
    "sequences = []\n",
    "for i in range(length, len(char_text)): \n",
    "    seq = char_text[i-length:i]\n",
    "    # Seq stored as single string at this point\n",
    "    seq_string = ''.join(seq)\n",
    "    sequences.append(seq_string)\n",
    "# Print the number of sequences constructed \n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters for creating a character embedding\n",
    "unique_chars = sorted(list(set(process_string)))\n",
    "# Map to integers \n",
    "mapping = dict((c, i) for i, c in enumerate(unique_chars))\n",
    "# Our total vocabulary size will be the unique characters\n",
    "vocab_size = len(unique_chars)\n",
    "# See what our mapping is \n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Store processed characters in a text file for easier later access\n",
    "with open(\"process_shakespeare_final.txt\", \"wb\") as fp: \n",
    "    pickle.dump(char_text, fp)\n",
    "# Store mapping for later access\n",
    "with open(\"mapping.pkl\", \"wb\") as fp: \n",
    "    pickle.dump(mapping, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all individual characters to mapped integers\n",
    "num_char = []\n",
    "for line in sequences: \n",
    "    encoded = [mapping[char] for char in line]\n",
    "    num_char.append(encoded)\n",
    "# Verify that the seq is still of length 41\n",
    "print(len(num_char[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and labels\n",
    "num_char = np.array(num_char)\n",
    "X, y = num_char[:, :-1], num_char[:, -1]\n",
    "# See what sizes our matrices are \n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode integers\n",
    "X = np.array([to_categorical(x, num_classes=len(mapping)) for x in X])\n",
    "y = to_categorical(y, num_classes=len(mapping))\n",
    "print(X.shape)\n",
    "# See what sizes our matrices are\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Store the ready-to-run version of X and y for later access \n",
    "# Note: file size usually too large to upload to Google Colab directly\n",
    "with open(\"X_training_shakespeare_final.h5\", \"wb\") as fp: \n",
    "    pickle.dump(X, fp)\n",
    "with open(\"y_training_shakespeare_final.h5\", \"wb\") as fp: \n",
    "    pickle.dump(y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LSTM model (Model 3 + Model 4 architecture)\n",
    "# Note: see report for more details on implementations related to model number\n",
    "# LSTM has 120 units\n",
    "# Dense layer has softmax layers equal to vocab_size for predicting each character\n",
    "model = Sequential()\n",
    "model.add(LSTM(120, input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 training conditions\n",
    "# Completed in one run\n",
    "# Total 40 epochs\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs = 40, batch_size = 8, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
