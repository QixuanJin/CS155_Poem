{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pickle import dump\n",
    "from pickle import load \n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ready-to-run shakespeare training data \n",
    "X = load(open(\"X_training_shakespeare_final.h5\", \"rb\"))\n",
    "y = load(open(\"y_training_shakespeare_final.h5\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw(filename): \n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess spenser text similar to shakespeare data\n",
    "file2 = 'spenser.txt'\n",
    "raw_text2 = load_raw(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "process_text2 = re.sub(' +', ' ', raw_text2)\n",
    "# Remove general headers\n",
    "process_text2 = re.sub('\\n\\n.+\\n\\n', '', process_text2)\n",
    "# Remove first header\n",
    "process_text2 = re.sub('I\\n\\n', '', process_text2)\n",
    "# Split on newlines\n",
    "process_text2 = re.split(\"\\n        |\\n\", process_text2)\n",
    "# Remove any empty strings resulting from split\n",
    "process_text2 = list(filter(None, process_text2))\n",
    "# Check result\n",
    "print(process_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_string2 = \"\".join(process_text2)\n",
    "char_text2 = list(process_string2)\n",
    "char_text2 = list(filter(None, char_text2))\n",
    "print(len(char_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters for creating a character embedding\n",
    "unique_chars2 = sorted(list(set(process_string2)))\n",
    "unique_chars2.remove('')\n",
    "# Map to integers \n",
    "mapping2 = dict((c, i) for i, c in enumerate(unique_chars2))\n",
    "vocab_size2 = len(unique_chars2)\n",
    "# See our mapping\n",
    "print(mapping2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the character mapping for shakespeare \n",
    "with open(\"mapping.pkl\", \"rb\") as fp: \n",
    "    mapping = pickle.load(fp)\n",
    "vocab_size = len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we decided to use only the shakespeare mapping \n",
    "# for both data sets. We see which extra characters are present\n",
    "# in Spenser sonnets and remove them. The number of removals is \n",
    "# not significant. \n",
    "diff = list(set(unique_chars2) - set(unique_chars))\n",
    "print(diff)\n",
    "print(char_text2.count('&'))\n",
    "print(char_text2.count('X'))\n",
    "print(char_text2.count('Q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove and make sure no longer in text \n",
    "char_text2.remove('&')\n",
    "char_text2.remove('X')\n",
    "char_text2.remove('Q')\n",
    "char_text2.remove('Q')\n",
    "char_text2.remove('Q')\n",
    "print(char_text2.count('&'))\n",
    "print(char_text2.count('X'))\n",
    "print(char_text2.count('Q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo mapping to verify \n",
    "# Get all unique characters for creating a character embedding\n",
    "unique_chars2 = sorted(list(set(process_string2)))\n",
    "unique_chars2.remove('')\n",
    "# Map to integers \n",
    "mapping2 = dict((c, i) for i, c in enumerate(unique_chars2))\n",
    "vocab_size2 = len(unique_chars2)\n",
    "# See our mapping\n",
    "print(mapping2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also parse into sequences of 40 (training) + 1 (prediction)\n",
    "length2 = 40 + 1\n",
    "sequences2 = []\n",
    "for i in range(length2, len(char_text2)): \n",
    "    seq = char_text2[i-length2:i]\n",
    "    seq_string = ''.join(seq)\n",
    "    sequences2.append(seq_string)\n",
    "print(len(sequences2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of characters to corresponding integer\n",
    "num_char2 = []\n",
    "for line in sequences2: \n",
    "    encoded = [mapping[char] for char in line]\n",
    "    num_char2.append(encoded)\n",
    "print(len(num_char2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "num_char2 = np.array(num_char2)\n",
    "X2, y2 = num_char2[:, :-1], num_char2[:, -1]\n",
    "print(X2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot one encode\n",
    "X2 = np.array([to_categorical(x, num_classes=len(mapping)) for x in X2])\n",
    "y2 = to_categorical(y2, num_classes=len(mapping))\n",
    "print(X2.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our processed character file for later access\n",
    "with open(\"process_spenser.txt\", \"wb\") as fp: \n",
    "    pickle.dump(char_text2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ready-to-run X and y data for spenser sonnets\n",
    "with open(\"X_training_spenser.h5\", \"wb\") as fp: \n",
    "    pickle.dump(X2, fp)\n",
    "with open(\"y_training_spenser.h5\", \"wb\") as fp: \n",
    "    pickle.dump(y2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the new spenser training data on top of the shakespeare data\n",
    "X_train = np.vstack((X2, X))\n",
    "print(X_train.shape)\n",
    "y_train = np.vstack((y2, y))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue training directly from Model 3\n",
    "model = load_model(\"model3.h5\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 training conditions \n",
    "# Saved model every 5 epochs\n",
    "# Total 20 epochs\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size = 64, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
